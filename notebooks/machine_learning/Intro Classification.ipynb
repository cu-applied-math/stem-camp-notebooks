{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Intro Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cu-applied-math/stem-camp-notebooks/blob/master/notebooks/machine_learning/Intro%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEsos4f895DF",
        "colab_type": "text"
      },
      "source": [
        "# Intro to Classification Lab 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyuAV2tX95DG",
        "colab_type": "text"
      },
      "source": [
        "In this lab we will learn how to generate synthetic data and how to apply various built-in classifiers to classify the data. The goal of this lab is to introduce you to a subset of classification methods and which methods perform better depending on the structure of the data. We will also see where data visulaization and background information can come in handy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oSbhNDj95DH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import base libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL2ee3Ml95DK",
        "colab_type": "text"
      },
      "source": [
        "Python's scikit-learn (or sklearn) is a Machine Learning library equipped with simple and efficient tools for data mining and data analysis. In general, a learning problem consists of a set of $n$ samples of data and then tries to predict properties of _unknown_ data.\n",
        "\n",
        "If each sample is **more** than a single number (aka multivariate), it is said to have several attributes or **features**. \n",
        "\n",
        "We will mainly focus on **supervised** learning, in which each input data has an associated label. If we have time, we will discuss unsupervised learning and what techniques are commonly used today."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2GUEMff95DL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "\n",
        "X, y = make_classification(n_classes = 2, n_samples = 100, \\\n",
        "                           n_features = 2, n_redundant = 0, \\\n",
        "                           random_state = 1, n_clusters_per_class = 1)\n",
        "\n",
        "rng = np.random.RandomState(2)\n",
        "X += 2 * rng.uniform(size=X.shape) # add some noise to the data\n",
        "linearly_separable = (X, y)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SJq2xgq95DN",
        "colab_type": "text"
      },
      "source": [
        "*make_classification* generates a random classification problem where the number of class is user-specified. Later we will see *make_moons* and *make_circles*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF2t70ci95DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure = plt.figure( figsize=(27,9) )\n",
        "color_map = plt.cm.RdBu #Red-Blue colormap\n",
        "cm_bright = ListedColormap(['#FF0000','#0000FF'])\n",
        "\n",
        "X = StandardScaler().fit_transform(X)# center and scale the data\n",
        "\n",
        "# Split the data to reserve some for model validation\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 # for plotting\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 # y-axis, not to be confused with y labels\n",
        "\n",
        "# Don't worry about the mesh grid now, we will use it to make pretty plots!\n",
        "h = 0.02 # the mesh step size\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "\n",
        "plt.title(\"(Make Classification) Input Data\", fontsize = 28)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
        "\n",
        "plt.axis([x_min, x_max, y_min, y_max])\n",
        "plt.grid(True)\n",
        "plt.xlabel('x', fontsize = 28), plt.ylabel('y', fontsize = 28)\n",
        "plt.tick_params(labelsize = 20)\n",
        "plt.show()\n",
        "\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMbglZ6195DQ",
        "colab_type": "text"
      },
      "source": [
        "# $K$-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofus2Lr195DQ",
        "colab_type": "text"
      },
      "source": [
        "The first classifier we are going to consider is $k$-nearest neighbors. The idea behind the method is that the input consists of the $k$ closest training examples in the feature space. An object is classified by a **majority vote** of its neighbors, with the object being assignmed to the class most common among its **_k_** nearest neighbors, hence the name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShsA3app95DR",
        "colab_type": "text"
      },
      "source": [
        "**Activity! Get up!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLgMnpPL95DR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "names = [\"Nearest Neighbors\"]\n",
        "#cl = [KNeighborsClassifier(3,weights = 'distance')] # Let's choose k=3\n",
        "cl = [KNeighborsClassifier(3)] # Let's choose k=3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4uVhcjI95DU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cl[0].fit(X_train,y_train)\n",
        "score = cl[0].score(X_test,y_test)\n",
        "\n",
        "# Plot the decision boundary for which we will assign a color to each class\n",
        "# concatonate vectorized grid and compute probability estimates for the data\n",
        "\n",
        "Z = cl[0].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1] # with confidence bds\n",
        "#Z = cl[0].predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
        "plt.title('Decision Boundary', fontsize = 18)\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsU0efjB95DW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding the data...\n",
        "plt.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
        "plt.title(names[0], fontsize = 20)\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWOAKXPw95DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#np.set_printoptions(threshold='nan')\n",
        "#cl[0].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1] # with confidence bds\n",
        "print(score) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogFWoFfG95Db",
        "colab_type": "text"
      },
      "source": [
        "Not bad! We see that the $k$-nearest neighbors algorithm was able to classify the unseen data with an accuracy of 90%!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTe2q4dg95Dc",
        "colab_type": "text"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM526i5o95Dc",
        "colab_type": "text"
      },
      "source": [
        "Next, we will take a look at Decision Tree Classification. A decision tree is a flowchart-like structure in which each internal **node** represents a \"test\" on an attribute (or feature), each **branch** represents the outcome of the test, and each **leaf** node represents a class label. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7szmp6Eq95Dc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "5e03aefd-a44e-416f-c1b1-0b7fb3a510b8"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://image.slidesharecdn.com/decisiontree-151015165353-lva1-app6892/95/classification-using-decision-tree-12-638.jpg?cb=1444928106')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://image.slidesharecdn.com/decisiontree-151015165353-lva1-app6892/95/classification-using-decision-tree-12-638.jpg?cb=1444928106\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6GnaqkP95Dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "names.append(\"Decision Tree\")\n",
        "cl.append(DecisionTreeClassifier(max_depth=5))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYMdXHsR95Dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cl[1].fit(X_train,y_train)\n",
        "score = cl[1].score(X_test,y_test)\n",
        "\n",
        "# Plot the decision boundary for which we will assign a color to each class\n",
        "# concatonate vectorized grid and compute probability estimates for the data\n",
        "\n",
        "#Z = cl[1].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1] # with confidence bds\n",
        "Z = cl[1].predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
        "plt.title('Decision Boundary', fontsize = 18)\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1OhiM5Q95Dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding the data...\n",
        "plt.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
        "plt.title(names[1], fontsize = 20)\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz8Cwwlb95Dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(score) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0S_HVtn95Dp",
        "colab_type": "text"
      },
      "source": [
        "Even better! A Decision Tree Classifier was able to predict the unseen data labels with an accuracy of over 93%!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVB_xNKo95Dq",
        "colab_type": "text"
      },
      "source": [
        "# Different Structured Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CP1MFJJ95Dq",
        "colab_type": "text"
      },
      "source": [
        "Now let's take a look at how these algorithms perform on data with different structural relationships. As you may think, sklearn's *make_moons* can artificial generate data whose class labeling's form \"moons\" around each other. Similarily, *make_circles* generates data grouped with circular structure. \n",
        "\n",
        "The cells below provide a visual of the what these functions output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS8V3CkH95Dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = make_moons(noise = 0.3, random_state = 0, n_samples = 200)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "\n",
        "plt.title(\"Make Moons Dataset\", fontsize = 28)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
        "\n",
        "plt.axis([x_min, x_max, y_min, y_max])\n",
        "plt.grid(True)\n",
        "plt.xlabel('x', fontsize = 28), plt.ylabel('y', fontsize = 28)\n",
        "plt.tick_params(labelsize = 20)\n",
        "plt.show()\n",
        "\n",
        "plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOCtDY2495Dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = plt.subplot(1,2,1)\n",
        "\n",
        "# Fit make_moons dataset with k-Nearest Neighbors Classifier\n",
        "cl[0].fit(X_train,y_train)\n",
        "score = cl[0].score(X_test,y_test)\n",
        "\n",
        "Z = cl[0].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "ax.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
        "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
        "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
        "ax.set_title(names[0], fontsize = 20)\n",
        "\n",
        "# Fit make_circles dataset with Decision Tree Classifier\n",
        "ax2 = plt.subplot(1,2,2)\n",
        "cl[1].fit(X_train,y_train)\n",
        "score2 = cl[1].score(X_test,y_test)\n",
        "\n",
        "Z = cl[1].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "ax2.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
        "ax2.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
        "ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
        "ax2.set_title(names[1], fontsize = 20)\n",
        "\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(\"Nearest Neighbors Score:\",score)\n",
        "print(\"Decision Tree Score:\",score2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxDswFAY95Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = make_circles(noise = 0.2, factor = 0.5, random_state = 1,n_samples = 200)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
        "    \n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "plt.title(\"Make Circles Dataset\", fontsize = 28)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
        "\n",
        "plt.axis([x_min, x_max, y_min, y_max])\n",
        "plt.grid(True)\n",
        "plt.xlabel('x', fontsize = 28), plt.ylabel('y', fontsize = 28)\n",
        "plt.tick_params(labelsize = 20)\n",
        "plt.show()\n",
        "\n",
        "plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ88xujQ95Dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = plt.subplot(1,2,1)\n",
        "\n",
        "# Fit make_circles dataset with k-Nearest Neighbors Classifier\n",
        "cl[0].fit(X_train,y_train)\n",
        "score = cl[0].score(X_test,y_test)\n",
        "\n",
        "Z = cl[0].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "ax.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
        "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
        "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
        "ax.set_title(names[0], fontsize = 20)\n",
        "\n",
        "# Fit make_circles dataset with Decision Tree Classifier\n",
        "ax2 = plt.subplot(1,2,2)\n",
        "cl[1].fit(X_train,y_train)\n",
        "score2 = cl[1].score(X_test,y_test)\n",
        "\n",
        "Z = cl[1].predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "ax2.contourf(xx,yy,Z, cmap = color_map, alpha= 0.8)\n",
        "ax2.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
        "ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.4)\n",
        "ax2.set_title(names[1], fontsize = 20)\n",
        "\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(\"Nearest Neighbors Score:\",score)\n",
        "print(\"Decision Tree Score:\",score2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
